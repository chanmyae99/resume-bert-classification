{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose and Scope of Text Pre-processing\n",
        "\n",
        "Text pre-processing is a critical step in Natural Language Processing pipelines,\n",
        "particularly when using transformer-based models such as BERT.\n",
        "\n",
        "Although BERT is robust to unstructured and noisy text, certain text quality issues\n",
        "including excessive formatting artifacts, inconsistent capitalization, and structural\n",
        "noise can negatively affect tokenization, attention distribution, and contextual\n",
        "representation.\n",
        "\n",
        "In this notebook, only **minimal and model-appropriate preprocessing techniques** are\n",
        "applied to address the significant text quality issues identified earlier. Aggressive\n",
        "text cleaning methods such as stemming, lemmatization, stopword removal, or text\n",
        "augmentation are intentionally avoided, as they may remove important contextual\n",
        "information or violate project constraints.\n",
        "\n",
        "All preprocessing steps preserve the original meaning of the resume text and do not\n",
        "introduce any external or synthetic content.\n"
      ],
      "metadata": {
        "id": "D2fT6kVlClP1"
      },
      "id": "D2fT6kVlClP1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "UORzkfqtEHP3"
      },
      "id": "UORzkfqtEHP3",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset\n",
        "\n",
        "\n",
        "The resume dataset is loaded in read-only mode.\n",
        "At this stage, no columns are modified, removed, or encoded.\n",
        "The `Category` column is preserved exactly as provided."
      ],
      "metadata": {
        "id": "8UGCGGXtD0aB"
      },
      "id": "8UGCGGXtD0aB"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chanmyae99/resume-bert-classification.git"
      ],
      "metadata": {
        "id": "Ap9SCy9IEA7H",
        "outputId": "6ac47278-2e62-4818-8bdf-c7faf37e05fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ap9SCy9IEA7H",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'resume-bert-classification'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 64 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (64/64), 930.92 KiB | 2.68 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd resume-bert-classification/notebooks"
      ],
      "metadata": {
        "id": "TD_Ab4oeD1D9",
        "outputId": "ee43c26f-a518-47ba-ee9f-6e16fb1c2876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TD_Ab4oeD1D9",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/resume-bert-classification/notebooks/resume-bert-classification/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../data/raw/CHAN MYAE AUNG.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FIYvT1i7D6t2",
        "outputId": "a82a2ab2-d29d-4e60-d722-2c88a75d3185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "FIYvT1i7D6t2",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Category  \\\n",
              "0           Python Developer   \n",
              "1         Health and fitness   \n",
              "2               Data Science   \n",
              "3  Network Security Engineer   \n",
              "4             Java Developer   \n",
              "\n",
              "                                              Resume  \n",
              "0  Technical Skills: Languages Python Python Fram...  \n",
              "1  Education Details \\r\\nJanuary 2018 M.S. Nutrit...  \n",
              "2  Education Details \\r\\n MCA   YMCAUST,  Faridab...  \n",
              "3  Operating Systems: Windows, Linux, Ubuntu Netw...  \n",
              "4  Education Details \\r\\n BE IT   pjlce\\r\\nJava D...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17d67018-33fd-44d2-a818-9d5ca00b4091\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Resume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Python Developer</td>\n",
              "      <td>Technical Skills: Languages Python Python Fram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Health and fitness</td>\n",
              "      <td>Education Details \\r\\nJanuary 2018 M.S. Nutrit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Science</td>\n",
              "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Network Security Engineer</td>\n",
              "      <td>Operating Systems: Windows, Linux, Ubuntu Netw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Java Developer</td>\n",
              "      <td>Education Details \\r\\n BE IT   pjlce\\r\\nJava D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17d67018-33fd-44d2-a818-9d5ca00b4091')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-17d67018-33fd-44d2-a818-9d5ca00b4091 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-17d67018-33fd-44d2-a818-9d5ca00b4091');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 962,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Business Analyst\",\n          \"Mechanical Engineer\",\n          \"Python Developer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Resume\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 218,\n        \"samples\": [\n          \"COMPUTER PROFICIENCY \\u00e2\\u0080\\u00a2 Basic: MS-Office (PowerPoint, word, Outlook, Excel) \\u00e2\\u0080\\u00a2 Language Known: Basics of C, CPP, Java. \\u00e2\\u0080\\u00a2 Basics of Networking \\u00e2\\u0080\\u00a2 Basics command of Linux PROJECT DETAILS Minor Project Details: \\u00e2\\u0080\\u00a2 Title: Applocker for Android. \\u00e2\\u0080\\u00a2 Project Area: Android Application. \\u00e2\\u0080\\u00a2 Description: Applocker provides the protection of the System applications as well as the Third party applications installed in the Android devices. The password protection is provided with the help of patterns of volume keys. Hence, an extra measure of privacy is acquired. Major Project Details: \\u00e2\\u0080\\u00a2 Title: Online Complaint System For Cyber Crimes. \\u00e2\\u0080\\u00a2 Project Area: Android Application \\u00e2\\u0080\\u00a2 Description: Online Complaint System for Cyber Crimes is an android application which will be in use after a person lodged a complaint in Police station regarding cyber crime but no action has been taken against it within the prescribed time constraint. Such person will directly use this application which will help him/her to directly lodge the complaint to Commissioner Office and it will get store in the Commissioner's database and necessary action will be taken against it. STRENGTHS \\u00e2\\u0080\\u00a2 Belief in team work both as a team member and a leader. \\u00e2\\u0080\\u00a2 Hard and ethical worker.Education Details \\r\\nJanuary 2013 to January 2016 B.E. Yeshwantrao Chavan Nagpur, Maharashtra Nagpur University\\r\\n Diploma Aggregate  Maharashtra State\\r\\n S.S.C. Education Nagpur, Maharashtra Maharashtra State\\r\\nTesting engineer \\r\\n\\r\\n\\r\\nSkill Details \\r\\nANDROID- Exprience - 6 months\\r\\nCPP- Exprience - 6 months\\r\\nDATABASE- Exprience - 6 months\\r\\nEXCEL- Exprience - 6 months\\r\\nJAVA- Exprience - 6 months\\r\\nSelenium- Exprience - 12 months\\r\\nAutomation Testing- Exprience - 12 months\\r\\nSelenium Webdriver- Exprience - 12 months\\r\\nManual Testing- Exprience - 6 months\\r\\nRegression Testing- Exprience - 6 monthsCompany Details \\r\\ncompany - Maxgen technologies\\r\\ndescription - I'm a software test engineer working at Maxgen technologies from past 1 year.\",\n          \"Technical Skills / Responsibilities: \\u00e2\\u0080\\u00a2 Hands on Experience with Production and Maintenance of Projects. \\u00e2\\u0080\\u00a2 Experience in handling projects in agile methodology. \\u00e2\\u0080\\u00a2 Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. \\u00e2\\u0080\\u00a2 Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. \\u00e2\\u0080\\u00a2 Having strong knowledge of Debugging and Testing based on Python and AS/400. \\u00e2\\u0080\\u00a2 Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. \\u00e2\\u0080\\u00a2 Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: \\u00e2\\u0080\\u00a2 Completed Internal Python training. \\u00e2\\u0080\\u00a2 Completed Internal Python Web Crawling training. \\u00e2\\u0080\\u00a2 Completed Internal Python Web Scraping training. \\u00e2\\u0080\\u00a2 Completed Internal Python for Data Science training. \\u00e2\\u0080\\u00a2 Completed Internal MongoDB training. \\u00e2\\u0080\\u00a2 Completed Internal MySQL training. \\u00e2\\u0080\\u00a2 Completed Internal PostgreSQL training. \\u00e2\\u0080\\u00a2 Completed Internal DJango training. \\u00e2\\u0080\\u00a2 Completed Internal Angular 6, HTML, CSS training. \\u00e2\\u0080\\u00a2 Completed German A1 level and preparing for A2 from Goethe-Institute. \\u00e2\\u0080\\u00a2 Completed Internal Core Java training. \\u00e2\\u0080\\u00a2 Completed IBM I series AS\\\\400 Training course at Maples Institute, Pune. \\u00e2\\u0080\\u00a2 Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) \\u00e2\\u0080\\u00a2 Completed Internal M3 ERP training (Techn: Java) \\u00e2\\u0080\\u00a2 Completed Internal Stream serve training. \\u00e2\\u0080\\u00a2 Completed M3 Enterprise Collaborator (MEC) training.Education Details \\r\\n M.Sc. Computer Science Pune, Maharashtra Pune University\\r\\n B.Sc. Computer Science Pune, Maharashtra Pune University\\r\\n H.S.C.  Pune, Maharashtra Pune University\\r\\nPython RESTful API developer \\r\\n\\r\\nPython developer - KPIT Technologies\\r\\nSkill Details \\r\\nFlask- Exprience - Less than 1 year months\\r\\nPython- Exprience - Less than 1 year months\\r\\nRestful- Exprience - Less than 1 year months\\r\\nRest- Exprience - Less than 1 year months\\r\\nNumpy- Exprience - Less than 1 year months\\r\\nAS/400- Exprience - 90 monthsCompany Details \\r\\ncompany - KPIT Technologies\\r\\ndescription - since 6th July 2011 to till date:\\r\\n\\r\\n\\u00e2\\u0080\\u00a2 Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.\\r\\n\\u00e2\\u0080\\u00a2 Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1\\r\\nyear.\\r\\n\\u00e2\\u0080\\u00a2 Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.\\r\\n\\r\\nTechnical Expertise:\\r\\n\\u00e2\\u0080\\u00a2 Python development:\\r\\n\\u00e2\\u0080\\u00a2 Python - MongoDB\\r\\n\\u00e2\\u0080\\u00a2 Python - MySql\\r\\n\\u00e2\\u0080\\u00a2 Python Cache & Memoization\\r\\n\\u00e2\\u0080\\u00a2 Python GIT\\r\\n\\u00e2\\u0080\\u00a2 Python PWS (Pivotal Web Service - Cloud Foundry)\\r\\n\\u00e2\\u0080\\u00a2 German A1 Level\\r\\n\\r\\n\\u00e2\\u0080\\u00a2 M3/Movex ERP development:\\r\\n\\u00e2\\u0080\\u00a2 M3 Java of Movex/M3 ERP\\r\\n\\u00e2\\u0080\\u00a2 AS400 development of Movex/M3 ERP\\r\\n\\u00e2\\u0080\\u00a2 Stream Server development of Movex/M3 ERP\\r\\n\\u00e2\\u0080\\u00a2 Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF\\r\\n\\u00e2\\u0080\\u00a2 Movex/M3 Flows, Programs & database structure, MI Programs.  A7sG2lN9J0\",\n          \"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details \\r\\n M.C.A  Pune, MAHARASHTRA, IN Pune University\\r\\nHodoop Developer \\r\\n\\r\\nHodoop Developer - PRGX India Private Limited Pune\\r\\nSkill Details \\r\\nCompany Details \\r\\ncompany - PRGX India Private Limited Pune\\r\\ndescription - Team Size: 10+\\r\\nEnvironment: Hive, Spark, Sqoop, Scala and Flume.\\r\\n\\r\\nProject Description:\\r\\nThe bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.\\r\\n\\r\\nRole & Responsibilities:\\r\\n\\u00e2\\u009d\\u0096 Import data from legacy system to hadoop using Sqoop, flume.\\r\\n\\u00e2\\u009d\\u0096 Implement the business logic to analyses  the data\\r\\n\\u00e2\\u009d\\u0096 Per-process data using spark.\\r\\n\\u00e2\\u009d\\u0096 Create hive script and loading data into hive.\\r\\n\\u00e2\\u009d\\u0096 Sourcing various attributes to the data processing logic to retrieve the correct results.\\r\\n\\r\\nProject 2\\r\\ncompany - PRGX India Private Limited Pune\\r\\ndescription - \\r\\ncompany - PRGX India Private Limited Pune\\r\\ndescription - Team Size: 11+\\r\\nEnvironment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce\\r\\n\\r\\nProject Description:-\\r\\nThe Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing\\r\\n\\r\\nRole & Responsibilities:\\r\\n\\u00e2\\u009d\\u0096 Move all crawl data flat files generated from various micro sites to HDFS for further processing.\\r\\n\\u00e2\\u009d\\u0096 Sqoop implementation for interaction with database\\r\\n\\u00e2\\u009d\\u0096 Write Map Reduce scripts to process the data file.\\r\\n\\u00e2\\u009d\\u0096 Create hive tables to store the processed data in tabular formats.\\r\\n\\u00e2\\u009d\\u0096 Reports creation from hive data.\\r\\n\\r\\nProject 3\\r\\ncompany - PRGX India Private Limited Pune\\r\\ndescription - Team Size: 15+\\r\\nEnvironment: Informatica 9.5, Oracle11g, UNIX\\r\\n\\r\\nProject Description:\\r\\nPfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.\\r\\n\\r\\nRole & Responsibilities:\\r\\n\\u00e2\\u009d\\u0096 To understand & analyze the requirement documents and resolve the queries.\\r\\n\\u00e2\\u009d\\u0096 To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.\\r\\n\\u00e2\\u009d\\u0096 Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.\\r\\n\\u00e2\\u009d\\u0096 Create workflow/batches and set the session dependencies.\\r\\n\\u00e2\\u009d\\u0096 Implemented Change Data Capture using mapping parameters, SCD and SK generation.\\r\\n\\u00e2\\u009d\\u0096 Developed Mapplet, reusable transformations to populate the data into data warehouse.\\r\\n\\u00e2\\u009d\\u0096 Created Sessions & Worklets using workflow Manager to load the data into the Target Database.\\r\\n\\u00e2\\u009d\\u0096 Involved in Unit Case Testing (UTC)\\r\\n\\u00e2\\u009d\\u0096 Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.\\r\\n\\r\\nPersonal Scan\\r\\n\\r\\nAddress: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Copy\n",
        "\n",
        "To preserve the original resume text and enable clear before-and-after comparison,\n",
        "all preprocessing steps are applied to a copied version of the resume column.\n"
      ],
      "metadata": {
        "id": "6nODszBbF20e"
      },
      "id": "6nODszBbF20e"
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Resume_processed\"] = df[\"Resume\"].copy()\n"
      ],
      "metadata": {
        "id": "xhJUjmjrF3sd"
      },
      "id": "xhJUjmjrF3sd",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Normalisation\n",
        "\n",
        "To address inconsistent capitalization across resumes, all text is converted to lowercase.\n",
        "This reduces stylistic variation introduced by formatting conventions such as uppercase\n",
        "section headers while preserving semantic meaning.\n"
      ],
      "metadata": {
        "id": "buxPfCA-DTtS"
      },
      "id": "buxPfCA-DTtS"
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Resume_processed\"] = df[\"Resume_processed\"].str.lower()\n",
        "\n"
      ],
      "metadata": {
        "id": "3L1sbYlEC8Rz"
      },
      "id": "3L1sbYlEC8Rz",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = 0\n",
        "\n",
        "print(\"=== BEFORE CASE NORMALISATION ===\\n\")\n",
        "print(df[\"Resume\"].iloc[sample_index][:400])\n",
        "\n",
        "print(\"\\n=== AFTER CASE NORMALISATION ===\\n\")\n",
        "print(df[\"Resume_processed\"].iloc[sample_index][:400])\n",
        "\n"
      ],
      "metadata": {
        "id": "Lzey3-itEobr",
        "outputId": "a8bb5c58-5b37-494b-9b1d-78f3ce8aa03f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Lzey3-itEobr",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE CASE NORMALISATION ===\n",
            "\n",
            "Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details \r\n",
            " BE   Dr.BAMU,Aurangabad\r\n",
            "Python Developer \r\n",
            "\r\n",
            "Python Developer - Arsys Inovics pvt ltd\r\n",
            "Skill Details \r\n",
            "CSS- Exp\n",
            "\n",
            "=== AFTER CASE NORMALISATION ===\n",
            "\n",
            "technical skills: languages python python framework django, drf databases mysql, oracle, sqlite, mongodb web technologies css, html, restful web services rest methodologies agile, scrum version control github project managent tool jira operating systems window, unix education details \r\n",
            " be   dr.bamu,aurangabad\r\n",
            "python developer \r\n",
            "\r\n",
            "python developer - arsys inovics pvt ltd\r\n",
            "skill details \r\n",
            "css- exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removal of Special Characters and Symbols\n",
        "\n",
        "Special characters and symbols introduced by resume formatting do not contribute\n",
        "meaningful semantic information and may generate noisy tokens during BERT\n",
        "tokenization. These characters are removed to reduce noise.\n"
      ],
      "metadata": {
        "id": "5hsh1MKuEV2E"
      },
      "id": "5hsh1MKuEV2E"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    return re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "\n",
        "df[\"Resume_processed\"] = df[\"Resume_processed\"].apply(remove_special_characters)\n"
      ],
      "metadata": {
        "id": "xzSG_JRbGaOk"
      },
      "id": "xzSG_JRbGaOk",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== BEFORE SPECIAL CHARACTER REMOVAL ===\\n\")\n",
        "print(df[\"Resume\"].iloc[sample_index][1520: 1800])\n",
        "print(\"\\n\")\n",
        "print(\"=== AFTER SPECIAL CHARACTER REMOVAL ===\\n\")\n",
        "print(df[\"Resume_processed\"].iloc[sample_index][1520: 1800])\n"
      ],
      "metadata": {
        "id": "i7qXUP0gGh7d",
        "outputId": "3e388641-086f-417c-e452-cd3428c68569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i7qXUP0gGh7d",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE SPECIAL CHARACTER REMOVAL ===\n",
            "\n",
            "ities:\r\n",
            "â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.\r\n",
            "â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website\n",
            "\n",
            "\n",
            "=== AFTER SPECIAL CHARACTER REMOVAL ===\n",
            "\n",
            "ities \r\n",
            "    participated in entire lifecycle of the projects including design  development  and deployment  testing and implementation and support \r\n",
            "    developed views and templates with python and django s view controller and templating language to created user friendly website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalisation of Line Breaks and Whitespace\n",
        "\n",
        "Excessive line breaks and irregular spacing fragment sentence continuity.\n",
        "Whitespace is normalised to improve contextual flow for BERT’s attention mechanism.\n"
      ],
      "metadata": {
        "id": "qNs3Cba6IFCz"
      },
      "id": "qNs3Cba6IFCz"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(text):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Resume_processed\"] = df[\"Resume_processed\"].apply(normalize_whitespace)\n"
      ],
      "metadata": {
        "id": "9NRIVdPKIH0M"
      },
      "id": "9NRIVdPKIH0M",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== BEFORE WHITESPACE NORMALISATION ===\\n\")\n",
        "print(df[\"Resume\"].iloc[sample_index][:400])\n",
        "print(\"\\n\")\n",
        "print(\"=== AFTER WHITESPACE NORMALISATION ===\\n\")\n",
        "print(df[\"Resume_processed\"].iloc[sample_index][:400])\n"
      ],
      "metadata": {
        "id": "rE3jRbdiIK0f",
        "outputId": "c61a062b-1b3d-4791-821a-f97d3541d6f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rE3jRbdiIK0f",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE WHITESPACE NORMALISATION ===\n",
            "\n",
            "Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details \r\n",
            " BE   Dr.BAMU,Aurangabad\r\n",
            "Python Developer \r\n",
            "\r\n",
            "Python Developer - Arsys Inovics pvt ltd\r\n",
            "Skill Details \r\n",
            "CSS- Exp\n",
            "\n",
            "\n",
            "=== AFTER WHITESPACE NORMALISATION ===\n",
            "\n",
            "technical skills languages python python framework django drf databases mysql oracle sqlite mongodb web technologies css html restful web services rest methodologies agile scrum version control github project managent tool jira operating systems window unix education details be dr bamu aurangabad python developer python developer arsys inovics pvt ltd skill details css exprience 31 months django e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduction of Redundant Word Repetition\n",
        "\n",
        "Some resumes contain immediate repetition of skills due to formatting.\n",
        "Consecutive duplicate words are reduced to prevent attention bias while\n",
        "preserving semantic content.\n"
      ],
      "metadata": {
        "id": "t70EGPbpIdXg"
      },
      "id": "t70EGPbpIdXg"
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_redundant_repetition(text):\n",
        "    words = text.split()\n",
        "    cleaned = [words[0]] if words else []\n",
        "    for w in words[1:]:\n",
        "        if w != cleaned[-1]:\n",
        "            cleaned.append(w)\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "df[\"Resume_processed\"] = df[\"Resume_processed\"].apply(reduce_redundant_repetition)\n"
      ],
      "metadata": {
        "id": "fPrb412iIegj"
      },
      "id": "fPrb412iIegj",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== BEFORE ===\\n\")\n",
        "print(df[\"Resume\"].iloc[sample_index][:400])\n",
        "\n",
        "print(\"\\n=== AFTER ===\\n\")\n",
        "print(df[\"Resume_processed\"].iloc[sample_index][:400])\n"
      ],
      "metadata": {
        "id": "vxrj7tmNIg5s",
        "outputId": "4c6f306b-ad84-4560-e951-7814496d0fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vxrj7tmNIg5s",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE ===\n",
            "\n",
            "Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details \r\n",
            " BE   Dr.BAMU,Aurangabad\r\n",
            "Python Developer \r\n",
            "\r\n",
            "Python Developer - Arsys Inovics pvt ltd\r\n",
            "Skill Details \r\n",
            "CSS- Exp\n",
            "\n",
            "=== AFTER ===\n",
            "\n",
            "technical skills languages python framework django drf databases mysql oracle sqlite mongodb web technologies css html restful web services rest methodologies agile scrum version control github project managent tool jira operating systems window unix education details be dr bamu aurangabad python developer python developer arsys inovics pvt ltd skill details css exprience 31 months django exprienc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Text Pre-processing\n",
        "\n",
        "All preprocessing steps were applied selectively to address specific text quality\n",
        "issues identified earlier. The original resume text was preserved throughout the\n",
        "process, and before-and-after examples demonstrate the effect of each technique.\n",
        "\n",
        "The final preprocessed resume text is now suitable for BERT tokenization and\n",
        "model training.\n"
      ],
      "metadata": {
        "id": "CXbvZtVvIzAP"
      },
      "id": "CXbvZtVvIzAP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the folder exists\n",
        "!mkdir -p ../data/preprocessed\n",
        "\n",
        "# Save the preprocessed CSV\n",
        "df.to_csv(\"../data/preprocessed/resumes_preprocessed.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "KN_nehOxKAjD"
      },
      "id": "KN_nehOxKAjD",
      "execution_count": 43,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}